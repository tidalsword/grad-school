{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 1 - BIA 6304 - Corey Austen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due Date: 5:45pm, Wednesday, January 24\n",
    "Deliverable Format: HTML, PDF, or Word document(s). Turn in output from the tasks and the answers to the questions.  They can be in the same file or separate files. Uncompiled Notebooks (files with .ipynb) will NOT be accepted. \n",
    "\n",
    "This assignment is worth 19 points. Assignments turned in after the due date and time will lose 2 points for every day late. No assignments will be accepted after January 31.\n",
    "\n",
    "* Answer corresponding questions (denoted by Q#) by inserting markdown text or creating a separate text document. Questions are assessed as follows:\n",
    "\n",
    "•\tComplete answer using appropriate terminology and not adding unnecessary or incorrect text: 5 points.\n",
    "•\tMostly complete answer that may not use all language appropriately:  3 points\n",
    "•\tIncomplete answer or one that uses incorrect language: 1 point\n",
    "•\tIncorrect answer: 0 points\n",
    "\n",
    "Hint:  while concise answers are appreciated, please make sure your responses are in complete sentences and are at least one paragraph long. Single word responses or sentence fragments will be considered incomplete answers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Questions\n",
    "\n",
    "Q1.  Write a short description of what the data you have captured are and how they might be used. Make sure your answer is no longer than two paragraphs, and should at minimum answer these questions:\n",
    "•\tWhat information can we get from the text?  What question might you answer?  Why is that interesting?\n",
    "•\tWhat information besides the text (e.g. tweet or headline) might you need to answer your question? Why? \n",
    "Audience: general – management or non-technical staff.\n",
    "\n",
    "Q2. Write a short description of what the vector space represents and how you can use it. Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "\n",
    "•\tHow do the parameter settings affect the size of the feature space?\n",
    "•\tDoes using weights make sense for your question?  If so, why?  If not, why not?\n",
    "•\tWhat parameter settings might be “best” for the question you have in mind? \n",
    "\n",
    "Hint:  if you want to show numbers, a table would work well here.  \n",
    "Audience: technical – fellow data scientists or other technical staff.\n",
    "\n",
    "Q3. Besides emojis and encodings that we discussed in class, what other changes might you consider making to the text, if any?  What effect would you expect this to have? If you wouldn’t make any other changes, explain why not. \n",
    "Audience: general – management or non-technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "### A1. \n",
    "\n",
    "We can get a sense of what is going on in the world based on the most common words in the headlines.  You might be able to answer some questions concerning internet virality, like if an action taken by a company or organization appeared in the headlines, it could be used to gauge the reaction to news very quickly, etc. \n",
    "\n",
    "The information that you might need in order to really answer this question could be web traffic data.  Some websites or blogs could be posting a considerable amount of content related to a certain news story or regarding a company, but if that particular website has low web traffic, then it would not be an accurate representation of how popular a headline is.  Some websites have data somewhere on the page that shows how many times a page has been viewed, shared on Facebook, tweeted about, etc.  Gathering this alongside the test would be more beneficial than grabbing the text alone.\n",
    "\n",
    "### A2.\n",
    "\n",
    "The vector space represents the useable data that was mined from the corpus.  We can use the feature space to gather as a dataset for predictive models, as a way to find the most important feature in a group of documents, etc.  I created 4 different vectorizers to examine how they affect the size and feature variety within the feature space.  What I noticed by doing this is that by putting more restrictive parameters in the vectorizer, the feature space would shrink.  Parameters like min_df and max_df, when set to a small window, will drastically reduce size of the feature space.  Removing stop words will also reduce the features.  A wide window on min_df and max_df, or not setting them at all, will expand the feature space. Setting the ngram_range to more then one word will drastically increase the feature space, as single words and combinations of words are then included.\n",
    "\n",
    "Using weights in the vectorizers for this question would not make sense.  This is because the headlines are very short and we would be looking for how often a company or term appears in the headlines.  The weight would indicate how important a word is in each headline, but since they are so short and vary so much, the weights will be high for a lot of terms, as seen in the vectorizers 3 and 4 that I created below.  If we were really trying to answer this question (with a much larger corpus), the ideal parameters for this would probably look similar to this:\n",
    "\n",
    "optimal_vectorizer = CountVectorizer(binary=False, min_df = .05, max_df = .8, stop_words = \"english\", ngram_range = (1,2), lowercase = False)\n",
    "\n",
    "The min_df and max_df range is set this way to capture words that appear often enough to not be totally unique, but not ones that appear in every headline. Stop words would not be important.  Single words and two word phrases might be important here, as a positive phrase, negative phrase, or company name could be multiple words.  The lowercase setting would be useful here because the company name may be a word or phrase that consists of common terms, like Bank of America.\n",
    "\n",
    "\n",
    "### A3.\n",
    "\n",
    "The text was fairly easy to scrape, but one could make is to have the text say whether it was a sound bite or a video.  I'm sure this could be done by creating an if statement where it looks for a particular piece of HTML and enters a value into a dictionary.  I tried to do this with the subcategories, such as \"The Two-Way\" and \"Politics,\" as that can be seen in the HTML.  However, I was unable to figure out how to extract this text, as a python dictionary does not allow for duplicate keys. I should have been able to extract them into a list, but I was unsure how to code that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Create a data frame of news headlines similar to the example done in class.  The source can be any “reputable” news website.  Include the headline, the date, a category or classification, and one other descriptive term about the headline (e.g. short/long, opinion, includes video?, etc.) and show only the head and the tail of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import module(s) into namespace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I decided to use the \"News\" section on NPR.com\n",
    "\n",
    "http://www.npr.com/sections/news/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see what is going on on NPR.com\n",
    "page = requests.get('https://www.npr.org/sections/news/')\n",
    " \n",
    "soup = BeautifulSoup(page.text, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'll avoid printing out the HTML.\n",
    "#print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NPRobj = soup.find_all(\"h2\",{\"class\":\"title\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is where I am adding in the subtopics as seen on the website.  I tried to pull the data directly from the website, but I was getting caught up on the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>National Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shots - Health News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sub-Category\n",
       "0    National Security\n",
       "1          The Two-Way\n",
       "2          The Two-Way\n",
       "3  Shots - Health News\n",
       "4          The Two-Way"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "slug = [\"National Security\", \"The Two-Way\", \"The Two-Way\", \"Shots - Health News\", \"The Two-Way\", \"The Rise of the Contract Workers\", \"Monkey See\", \"The Two-Way\", \"The Two-Way\", \"Shots - Health News\", \"The Two-Way\", \"The Salt\", \"The Two-Way\", \"Politics\", \"The Two-Way\", \"Shots - Health News\", \"Shots - Health News\", \"The Two-Way\", \"The Two-Way\", \"The Two-Way\", \"The Two-Way\", \"The Two-Way\", \"The Rise of the Contract Workers\", \"The Torch\", \"Politics\"]\n",
    "nprdf2 = pd.DataFrame({'Sub-Category':slug}) #Creating a manual list of the added topics\n",
    "nprdf2.columns = ['Sub-Category']\n",
    "nprdf2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(len(NPRobj))\n",
    "headlines = {}\n",
    "\n",
    "for name in NPRobj:\n",
    "    headlines.setdefault(name.get_text(strip=True), [])\n",
    "    headlines[name.get_text(strip=True)].append(time.strftime(\"%m/%d/%Y\"))\n",
    "    headlines[name.get_text(strip=True)].append(\"Front Page News\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3)\n"
     ]
    }
   ],
   "source": [
    "# store the text in a useful format\n",
    "\n",
    "nprdf1 = pd.DataFrame.from_dict(headlines,orient=\"index\") #Creating a dataframe\n",
    "nprdf1.reset_index(level=[0], inplace=True) #Setting the index\n",
    "print(nprdf1.shape)\n",
    "\n",
    "nprdf1.columns = ['Headline', 'Date', 'Category'] #Setting Column Headers\n",
    "nprdf1 = nprdf1.replace('\\n','', regex=True) #Removing Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sessions Interviewed By Special Counsel Robert Mueller As Part Of Russia Inquiry</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>National Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shooting At Kentucky High School Leaves 2 Dead, At Least 17 Injured</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Motel 6 Sued For Identifying Latino Guests For Immigration Agents</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E-Cigarettes Likely Encourage Kids To Try Tobacco But May Help Adults Quit</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>Shots - Health News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ursula K. Le Guin, Whose Novels Plucked Truth From High Fantasy, Dies At 88</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           Headline  \\\n",
       "0  Sessions Interviewed By Special Counsel Robert Mueller As Part Of Russia Inquiry   \n",
       "1               Shooting At Kentucky High School Leaves 2 Dead, At Least 17 Injured   \n",
       "2                 Motel 6 Sued For Identifying Latino Guests For Immigration Agents   \n",
       "3        E-Cigarettes Likely Encourage Kids To Try Tobacco But May Help Adults Quit   \n",
       "4       Ursula K. Le Guin, Whose Novels Plucked Truth From High Fantasy, Dies At 88   \n",
       "\n",
       "         Date         Category         Sub-Category  \n",
       "0  01/24/2018  Front Page News    National Security  \n",
       "1  01/24/2018  Front Page News          The Two-Way  \n",
       "2  01/24/2018  Front Page News          The Two-Way  \n",
       "3  01/24/2018  Front Page News  Shots - Health News  \n",
       "4  01/24/2018  Front Page News          The Two-Way  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprdf = pd.concat([nprdf1, nprdf2],axis=1) #merging the two dataframes together\n",
    "nprdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Puerto Rico's Governor Announces Plan To Privatize Island's Troubled Electric Utility</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Trump Slaps Tariffs On Imported Solar Panels And Washing Machines</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Two-Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Will Work For No Benefits: The Challenges Of Being In The New Contract Workforce</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Rise of the Contract Workers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gus Kenworthy Will Be The Second Openly Gay Man To Compete For U.S. In Winter Games</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>The Torch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Trump Signs Funding Bill, Bringing Shutdown To An End</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>Front Page News</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 Headline  \\\n",
       "20  Puerto Rico's Governor Announces Plan To Privatize Island's Troubled Electric Utility   \n",
       "21                      Trump Slaps Tariffs On Imported Solar Panels And Washing Machines   \n",
       "22       Will Work For No Benefits: The Challenges Of Being In The New Contract Workforce   \n",
       "23    Gus Kenworthy Will Be The Second Openly Gay Man To Compete For U.S. In Winter Games   \n",
       "24                                  Trump Signs Funding Bill, Bringing Shutdown To An End   \n",
       "\n",
       "          Date         Category                      Sub-Category  \n",
       "20  01/24/2018  Front Page News                       The Two-Way  \n",
       "21  01/24/2018  Front Page News                       The Two-Way  \n",
       "22  01/24/2018  Front Page News  The Rise of the Contract Workers  \n",
       "23  01/24/2018  Front Page News                         The Torch  \n",
       "24  01/24/2018  Front Page News                          Politics  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task  2\n",
    "\n",
    "Use the default to convert to lower case. Generate a vector space model. \n",
    "Try various combinations of vectorizer settings like we did in class: changing case, eliminating stop words, \n",
    "using min and max document frequency settings, choosing n-grams, applying Tfidf-weights, etc. Try at least 4 different versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer Version 1\n",
    "\n",
    "This Vectorizer uses word counts, a min_df of 5%, and a max_df of 80%, and does not remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 27)\n",
      "     count\n",
      "to       8\n",
      "of       7\n",
      "the      6\n",
      "at       5\n",
      "for      5\n"
     ]
    }
   ],
   "source": [
    "#The first configuration!\n",
    "\n",
    "vectorv1 = CountVectorizer(binary=False, min_df = .05, max_df = .8) #define the transformation\n",
    "vectorv1_news = vectorv1.fit_transform(nprdf['Headline']) #apply the transformation\n",
    "print(vectorv1_news.shape)\n",
    "\n",
    "names = vectorv1.get_feature_names()   #create list of feature names\n",
    "count = np.sum(vectorv1_news.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) # create a dataframe from the list\n",
    "vector1df = count_df.sort_values(['count'], ascending = False)  #arrange by count instead\n",
    "print(vector1df.head(5)) #Show top 5 counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer Version 2\n",
    "\n",
    "This Vectorizer uses word counts, a min_df of 5%, a max_df of 80%, and removes stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3)\n",
      "       count\n",
      "says       3\n",
      "trump      3\n",
      "work       3\n"
     ]
    }
   ],
   "source": [
    "# Configuration 2!\n",
    "\n",
    "vectorv2 = CountVectorizer(binary=False, min_df = .1, max_df = .8, stop_words = \"english\") #define the transformation\n",
    "vectorv2_news = vectorv2.fit_transform(nprdf['Headline']) #apply the transformation\n",
    "print(vectorv2_news.shape)\n",
    "\n",
    "names = vectorv2.get_feature_names()   #create list of feature names\n",
    "count = np.sum(vectorv2_news.toarray(), axis = 0) # add up feature counts \n",
    "count2 = count.tolist()  # convert numpy array to list\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count']) # create a dataframe from the list\n",
    "vector2df = count_df.sort_values(['count'], ascending = False)  #arrange by count instead\n",
    "print(vector2df.head(5)) #Show top 5 counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer Version 3\n",
    "\n",
    "This Vectorizer uses weights, removes stop words, a min_df of 5%, and a max_df of 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 12)\n",
      "             count\n",
      "alert     3.159484\n",
      "contract  3.159484\n",
      "dies      3.159484\n",
      "funding   3.159484\n",
      "governor  3.159484\n"
     ]
    }
   ],
   "source": [
    "#Configuration 3!\n",
    "\n",
    "vectorv3 = TfidfVectorizer(use_idf=True, norm=None, stop_words = \"english\",\n",
    "                       min_df = .05, max_df = .7) #define the transformation\n",
    "vectorv3_dm = vectorv3.fit_transform(nprdf['Headline']) #apply the transformation\n",
    "print(vectorv3_dm.shape)\n",
    "\n",
    "names = vectorv3.get_feature_names() \n",
    "maxweight = np.max(vectorv3_dm.toarray(), axis = 0) # find the max weight of each feature \n",
    "maxweight2 = maxweight.tolist()  # convert numpy array to list\n",
    "\n",
    "maxweight_df = pd.DataFrame(maxweight2, index = names, columns = ['count']) # create a dataframe from the list\n",
    "maxweight_sort = maxweight_df.sort_values(['count'], ascending = False)\n",
    "print(maxweight_sort.head(5)) #Show the top 5 weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vectorizer Version 4\n",
    "\n",
    "This Vectorizer uses weights, removes stop words, a min_df of 5%, creates new features based on uppercase vs. lowercase words, and an ngram range of 1 to 2 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 29)\n",
      "             count\n",
      "At        5.297317\n",
      "For       5.297317\n",
      "The       4.932674\n",
      "After     3.159484\n",
      "Governor  3.159484\n",
      "Why       3.159484\n",
      "Plan      3.159484\n",
      "Part Of   3.159484\n",
      "Part      3.159484\n",
      "New       3.159484\n"
     ]
    }
   ],
   "source": [
    "#Configuration 4, this time with weights!\n",
    "vectorv4 = TfidfVectorizer(use_idf=True, norm=None, stop_words = \"english\",\n",
    "                         min_df=.05, ngram_range = (1,2), lowercase=False) #define the transformation\n",
    "vectorv4_dm = vectorv4.fit_transform(nprdf['Headline']) #apply the transformation\n",
    "print(vectorv4_dm.shape)\n",
    "\n",
    "names = vectorv4.get_feature_names() \n",
    "maxweight = np.max(vectorv4_dm.toarray(), axis = 0) # find the max weight of each feature \n",
    "maxweight2 = maxweight.tolist()  # convert numpy array to list\n",
    "\n",
    "maxweight_df = pd.DataFrame(maxweight2, index = names, columns = ['count']) # create a dataframe from the list\n",
    "maxweight_sort = maxweight_df.sort_values(['count'], ascending = False)\n",
    "print(maxweight_sort.head(10)) #Show the top 10 weights"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
