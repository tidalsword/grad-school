{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework Assignment 2 - BIA 6304 - Corey Austen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Write a short description of the context of the dataset in your own words. Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "•\tWhy did you choose the processing that you did? Give several specific examples. \n",
    "•\tWhat is the effect of the replacement on your feature space?  Does this make sense? Is it helpful for answering your question?  Why or why not?\n",
    " \n",
    " Audience: technical – fellow data scientists or other technical staff.\n",
    "\n",
    "#### A1. The dataset that I used was a csv file containing the text of Trump's State of the Union speech.  The each row is a line from the speech, consisting of a 1-2 sentences, making it 149 rows.  The question I am trying to answer is \"What did Trump focus on primarily in this speech?\"  I chose to process this dataset with a count vectorizer, a min_df of 3%, max_df of 50%, an ngram_range or 1 - 2 word phrase.  I chose to use a min_df of 3%, requiring a feature to appear in at least 4 documents, and a max_df of 50%, meaning that a feature is removed from the space if it appears in more than 74 documents.  Through playing with the min_df and max_df at various levels, I found that these tend to show the most valuable information, removing truly unique words, while at the same time throwing out those that are constantly being repeated. I set the n_gram to 1 - 2 words because this did manage to increase the feature space by 3 features, which were \"north korea\", \"last year\", and \"seong ho\". Ranges higher than this did not add to the feature space. I also chose to use a modified version of the NLTK stop word list, adding in a handful of words found in the dataset that gave no insight into what the speech was about, such as \"also\", \"home\", \"this\", and \"tonight\".\n",
    "\n",
    "#### To further process the data, I created a dictionary to replace several words to consolidate similar features, such as \"United States\" consolidating into \"america\", \"years\" into \"year\", and \"americans\" and \"us\" into \"american.\"  The effect of this is that it reduced the size of the feature space and showed how often the \"americans\" are mentioned.  This makes sense because trump uses several terms to identify the same thing, so separating them into separate features would cause that information to be lost.  This helps to answer the question because it shows that he talked about the american people very often in the speech.  I chose not to use a stemmer, because many of the features were chopped up and the meaning was lost.  This may have been useful in a larger corpus, but in this case it was not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Write a short description of how the sentiment analysis was done and what the outcome is. Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "•\tHow did your processing affect the sentiment assignment, if at all?\n",
    "•\tWhat measure did you use to determine the sentiment label?  Why?  Do any of the label assignments surprise you? \n",
    "•\tInclude a few specific examples of label assignment and how it was determined and why it does or does not make sense.\n",
    "\n",
    "Audience: general – management or non-technical staff.\n",
    "\n",
    "#### A2. The sentiment analysis was done by using the a_finn sentiment dictionary.  The outcome, for the most part, seemed to be accurate.  I also used the *** and *** sentiment dictionary on this corpus, but the a_finn dictionary appeared to be the most accurate.\n",
    "\n",
    "#### Processing didn't seem to make much of a difference on how the sentiment labels were assigned., with the exception of row 29 (index 28).  The processing actually changed the sentiment from \"Negative\" to \"Positive,\" which is how it should appear, which I thought was surprising.  Looking at the data, I would assume that the replacement of \"us\" with \"american\" had an effect on this.  The sentiment label was determined by finding words listed in the a_finn dictionary and adding the associated value to the sentiment count, with positive words adding to the count, negative words subtracting from the count.  If the count was greater than zero, the sentiment was \"Positive\", less than zero was \"Negative\", and equal to zero is \"Neutral\".  The value is based on the calculated sentiment intensity of that word. For example: \"good\" may have a value of 1, \"great\" may have a value of 2, and \"best\" may have a value of 3.  \n",
    "\n",
    "#### We can see how this works in index 4: \"each test has forged new american heroes to remind american who we are, and show american what we can be.\"  This was \"Positive\" because the sentiment dictionary includes the word \"heroes\", which raised the sentiment count by a value 2.  Another example of this can be seen in index 6: \"we saw strangers shielding strangers from a hail of gunfire on the las vegas strip.\"\tThe sentiment dictionary found \"hail\", with a value of 2 and \"gun\" with a value of -1.  This resulted in a \"Positive\" sentiment.  This is surprising because it actually came up with the correct sentiment.  I would have expected the sentiment label to be incorrect because \"hail of gunfire\" would normally be a very negative thing, but this dictionary has \"hail\" meaning praise, rather than something that would shower from the sky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Write a short description of the exercise and the outcome.  Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "•\tHow did you change the process to fix that outcome?\n",
    "•\tHow would you explain (justify, rationalize) those changes if necessary?\n",
    "\n",
    "Audience: general – management or non-technical staff.\n",
    "\n",
    "#### A3.  I found that the sentiment label in index 8 was incorrect, showing \"Neutral\" rather than \"Positive\":\n",
    "\n",
    "##### \"we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.\"\n",
    "\n",
    "#### In order to resolve this, I added an entry into the sentiment dictionary for the word \"rescue\" with a highly positive value of 3.  This brought the sentiment count over 0, so it is now \"Positive.\"  I would justify this by saying no other entries in the the data are affected, since rescue only appeared two other times and each of the documents it appeared in already had a sentiment of \"Positive.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Data science is all about analyzing data to draw a conclusion.  You have just made a change to your analysis to match a conclusion about a label that you already made. Write a short description defending your actions.  Is what you did right? What are the ethical issues involved, if any? What is your role as a data scientist? (Max 4 points)\n",
    "\n",
    "Audience: general – management or non-technical staff.\n",
    "\n",
    "#### A4. There are 3 times that \"rescue\" appears, with every other document receiving a \"Positive\" label.  Entering this into the dictionary only changed the outcome for this particular document, so I feel that the change would not be an issue of skewing the analysis.  I believe what I did here was right because the impact of the change I made was very minimal.  I could see a change like this being unethical if a data scientist made changes that drastically altered the outcome of the analysis or if they altered the analysis to intentionally sway the outcome. However, the change I made corrected an obvious mistake in the sentiment analysis, so I would not consider it wrong or unethical.  \n",
    "\n",
    "#### The role of a data scientist is to honestly analyze data, build a predictive model, and present the information in a clear and consise manner.  In my opinion, it should also be a priority for data scientists to change the data as little as possible.  Changes should be made to clean and clarify and data, but they should not be made to change or skew the message that the data is telling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T1.  Read in or create a data frame with at least one column of text to be analyzed.  This could be the text you used previously or new text. Based on the context of your dataset and the question you want to answer, identify at what processing you think is necessary (stop words, stemming, custom replacement, etc.) Compare the feature space before and after your processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import module(s) into namespace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "from __future__ import division\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in a csv file containing the text of Trump's State of the Union Address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname = \"/Users/ca034330/Google Drive/Corey - School/Spring 2018 A/BIA 6304 - Text Mining/HW_2/\"\n",
    "pd.set_option('display.max_colwidth', 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 1)\n",
      "['text']\n"
     ]
    }
   ],
   "source": [
    "speechdf = pd.read_csv(pathname + \"trump_speech.csv\", index_col = 0) \n",
    "print(speechdf.shape)\n",
    "print(list(speechdf)) #what does this code do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating the prebuilt stop word list and creating a custom stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the nltk stopword list and adding terms to it that appear too often and add no meaning.\n",
    "nltk_stopwords = stopwords.words(\"english\")\n",
    "skl_stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_nltk = nltk_stopwords + [\"000\", \"mr\", \"said\", \"says\",\"say\", \"ms\", \"also\", \"has\", \"this\", \"one\", \"home\", \"every\", \"tonight\" ]\n",
    "\n",
    "#nltk_stopwords.remove('before') #In case I need to remove words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the size of the feature space before word replacements and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 75)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americans</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count\n",
       "american      31\n",
       "america       27\n",
       "people        26\n",
       "us            24\n",
       "americans     24\n",
       "new           21\n",
       "year          19\n",
       "tax           15\n",
       "country       15\n",
       "last          13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=False, min_df = .03, max_df = .5,stop_words = custom_nltk, ngram_range = (1,2))\n",
    "cv_speech = cv.fit_transform(speechdf['text'])\n",
    "print(cv_speech.shape)\n",
    "\n",
    "names = cv.get_feature_names()\n",
    "count = np.sum(cv_speech.toarray(), axis = 0)\n",
    "count2 = count.tolist()\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "count_df.sort_values(['count'], ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary to replace words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a dictionary to replace words in the corpus in order to combine like terms.\n",
    "speech_dict = {'united states':'america', 'americans': 'american', 'years':'year', \"us\":\"american\"}\n",
    "\n",
    "\n",
    "def multiple_replace(dict, text): \n",
    "\n",
    "  text = str(text).lower()\n",
    "\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Congress, the first lady of the United States, and my fellow Americans:</td>\n",
       "      <td>mr. speaker, mr. vice president, members of congress, the first lady of the america, and my fellow american:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  text  \\\n",
       "0  Mr. Speaker, Mr. Vice President, members of Congress, the first lady of the United States, and my fellow Americans:   \n",
       "\n",
       "                                                                                                      cleantext  \n",
       "0  mr. speaker, mr. vice president, members of congress, the first lady of the america, and my fellow american:  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the dictionary to replace words in the corpus\n",
    "\n",
    "speechdf['cleantext'] = speechdf.text.apply(lambda x: multiple_replace(speech_dict, x))\n",
    "speechdf[0:1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Mr. Speaker, Mr. Vice President, members of Congress, the first lady of the United States, and my fellow Americans:\n",
      "Name: text, dtype: object\n",
      "~~~~~~~~~~~~~~~~~~~\n",
      "0    mr. speaker, mr. vice president, member of congress, the first ladi of the america, and my fellow american:\n",
      "Name: stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def stem_text(row):\n",
    "    text = str(row).split() \n",
    "    stemtext = [ps.stem(word) for word in text]\n",
    "    stem2text = ' '.join(stemtext)\n",
    "    return stem2text\n",
    "\n",
    "speechdf['stemmed'] = speechdf[\"cleantext\"].apply(lambda x: stem_text(x))\n",
    "print( speechdf.text[0:1])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~\")\n",
    "print(speechdf.stemmed[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 71)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "american     79\n",
       "america      33\n",
       "year         29\n",
       "people       26\n",
       "new          21\n",
       "tax          15\n",
       "country      15\n",
       "last         13\n",
       "congress     13\n",
       "great        13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=False, min_df = .03, max_df = .5,stop_words = custom_nltk, ngram_range = (1,2))\n",
    "cv_speech = cv.fit_transform(speechdf['cleantext'])\n",
    "print(cv_speech.shape)\n",
    "\n",
    "names = cv.get_feature_names()\n",
    "count = np.sum(cv_speech.toarray(), axis = 0)\n",
    "count2 = count.tolist()\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "count_df.sort_values(['count'], ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 97)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>american</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thi</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>america</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peopl</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ha</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "american     80\n",
       "thi          37\n",
       "america      33\n",
       "year         29\n",
       "new          21\n",
       "peopl        21\n",
       "ha           20\n",
       "hi           19\n",
       "wa           16\n",
       "tax          15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The stemmer in this case did not clear up the feature space, it only increased the size and added terms that make no sense.\n",
    "cv = CountVectorizer(binary=False, min_df = .03, max_df = .5,stop_words = custom_nltk, ngram_range = (1,3))\n",
    "cv_speech = cv.fit_transform(speechdf['stemmed'])\n",
    "print(cv_speech.shape)\n",
    "\n",
    "names = cv.get_feature_names()\n",
    "count = np.sum(cv_speech.toarray(), axis = 0)\n",
    "count2 = count.tolist()\n",
    "count_df = pd.DataFrame(count2, index = names, columns = ['count'])\n",
    "count_df.sort_values(['count'], ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not using a stemmer is the preferred method here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T2. Create a sentiment dictionary from one of the sources in class or find/create your own (potential bonus points for appropriate creativity). Using your dictionary, create sentiment labels for the text entries in your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the sentiment dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname = \"/Users/ca034330/Google Drive/Corey - School/Spring 2018 A/BIA 6304 - Text Mining/HW_2/Dictionaries/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading up the afinn sentiment dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afinn = {}\n",
    "for line in open(pathname+\"AFINN-111.txt\"):\n",
    "    tt = line.split('\\t')\n",
    "    afinn.update({tt[0]:int(tt[1])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def afinn_sent(inputstring):\n",
    "    \n",
    "    sentcount =0\n",
    "    for word in inputstring.split():  \n",
    "        if word.rstrip('?:!.,;') in afinn:\n",
    "            sentcount = sentcount + afinn[word.rstrip('?:!.,;')]\n",
    "            \n",
    "    \n",
    "    if (sentcount < 0):\n",
    "        sentiment = 'Negative'\n",
    "    elif (sentcount >0):\n",
    "        sentiment = 'Positive'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment\n",
    "    #return sentcount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechdf['afinn'] = speechdf[\"text\"].apply(lambda x: afinn_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechdf['afinn_clean'] = speechdf[\"cleantext\"].apply(lambda x: afinn_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleantext</th>\n",
       "      <th>afinn</th>\n",
       "      <th>afinn_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr. speaker, mr. vice president, members of congress, the first lady of the america, and my fellow american:</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>less than one year has passed since i first stood at this podium, in this majestic chamber, to speak on behalf of the american people — and to address their concerns, their hopes and their dreams. that night, our new administration had already taken swift action. a new tide of optimism was already sweeping across our land.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>each day since, we have gone forward with a clear vision and a righteoamerican mission — to make america great again for all american.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>over the last year, we have made incredible progress and achieved extraordinary success. we have faced challenges we expected, and others we could never have imagined. we have shared in the heights of victory and the pains of hardship. we endured floods and fires and storms. but through it all, we have seen the beauty of america's soul, and the steel in america's spine.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>each test has forged new american heroes to remind american who we are, and show american what we can be.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we saw the volunteers of the \"cajun navy,\" racing to the rescue with their fishing boats to save people in the aftermath of a devastating hurricane.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>we saw strangers shielding strangers from a hail of gunfire on the las vegas strip.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>we heard tales of american like coast guard petty officer ashlee leppert, who is here tonight in the gallery with melania. ashlee was aboard one of the first helicopters on the scene in hoamericanton during hurricane harvey. through 18 hours of wind and rain, ashlee braved live power lines and deep water to help save more than 40 lives. thank you, ashlee.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to everyone still recovering in texas, florida, louisiana, puerto rico, the virgin islands, california and everywhere else — we are with you, we love you, and we will pull through together.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                              cleantext  \\\n",
       "0                                                                                                                                                                                                                                                                          mr. speaker, mr. vice president, members of congress, the first lady of the america, and my fellow american:   \n",
       "1                                                  less than one year has passed since i first stood at this podium, in this majestic chamber, to speak on behalf of the american people — and to address their concerns, their hopes and their dreams. that night, our new administration had already taken swift action. a new tide of optimism was already sweeping across our land.   \n",
       "2                                                                                                                                                                                                                                                each day since, we have gone forward with a clear vision and a righteoamerican mission — to make america great again for all american.   \n",
       "3  over the last year, we have made incredible progress and achieved extraordinary success. we have faced challenges we expected, and others we could never have imagined. we have shared in the heights of victory and the pains of hardship. we endured floods and fires and storms. but through it all, we have seen the beauty of america's soul, and the steel in america's spine.   \n",
       "4                                                                                                                                                                                                                                                                             each test has forged new american heroes to remind american who we are, and show american what we can be.   \n",
       "5                                                                                                                                                                                                                                  we saw the volunteers of the \"cajun navy,\" racing to the rescue with their fishing boats to save people in the aftermath of a devastating hurricane.   \n",
       "6                                                                                                                                                                                                                                                                                                   we saw strangers shielding strangers from a hail of gunfire on the las vegas strip.   \n",
       "7                 we heard tales of american like coast guard petty officer ashlee leppert, who is here tonight in the gallery with melania. ashlee was aboard one of the first helicopters on the scene in hoamericanton during hurricane harvey. through 18 hours of wind and rain, ashlee braved live power lines and deep water to help save more than 40 lives. thank you, ashlee.   \n",
       "8                                                                                                                                                                      we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.   \n",
       "9                                                                                                                                                                                         to everyone still recovering in texas, florida, louisiana, puerto rico, the virgin islands, california and everywhere else — we are with you, we love you, and we will pull through together.   \n",
       "\n",
       "      afinn afinn_clean  \n",
       "0   Neutral     Neutral  \n",
       "1  Positive    Positive  \n",
       "2  Positive    Positive  \n",
       "3  Positive    Positive  \n",
       "4  Positive    Positive  \n",
       "5  Positive    Positive  \n",
       "6  Positive    Positive  \n",
       "7  Positive    Positive  \n",
       "8   Neutral     Neutral  \n",
       "9  Positive    Positive  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechdf.iloc[0:10][['cleantext','afinn','afinn_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>One of Staub's employees, Corey Adams, is also with us tonight. Corey is an all-American worker. He supported himself through high school, lost his job during the 2008 recession and was later hired by Staub, where he trained to become a welder. Like many hardworking Americans, Corey plans to invest his tax?cut raise into his new home and his two daughters' education. Please join me in congratulating Corey.</td>\n",
       "      <td>one of staub's employees, corey adams, is also with american tonight. corey is an all-american worker. he supported himself through high school, lost his job during the 2008 recession and was later hired by staub, where he trained to become a welder. like many hardworking american, corey plans to invest his tax?cut raise into his new home and his two daughters' education. please join me in congratulating corey.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "28  One of Staub's employees, Corey Adams, is also with us tonight. Corey is an all-American worker. He supported himself through high school, lost his job during the 2008 recession and was later hired by Staub, where he trained to become a welder. Like many hardworking Americans, Corey plans to invest his tax?cut raise into his new home and his two daughters' education. Please join me in congratulating Corey.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                         cleantext  \n",
       "28  one of staub's employees, corey adams, is also with american tonight. corey is an all-american worker. he supported himself through high school, lost his job during the 2008 recession and was later hired by staub, where he trained to become a welder. like many hardworking american, corey plans to invest his tax?cut raise into his new home and his two daughters' education. please join me in congratulating corey.  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the data changed row 28 from negative to positive. 'American' replaced 'us,' so maybe that had an effect.\n",
    "speechdf.iloc[28:29][['text','cleantext']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T3. Consider one of the entries in your corpus that had a surprising label.  How would you change your analysis to get the “right” label? Show specific results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleantext</th>\n",
       "      <th>afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          cleantext  \\\n",
       "8  we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.   \n",
       "\n",
       "     afinn  \n",
       "8  Neutral  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Line 9 (Index 8) should be \"positive,\" not \"neutral.\"\n",
    "speechdf.iloc[8:9][['cleantext',\"afinn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a new sentiment dictionary.\n",
    "new_afinn = {}\n",
    "for line in open(pathname+\"AFINN-111.txt\"):\n",
    "    tt = line.split('\\t')\n",
    "    new_afinn.update({tt[0]:int(tt[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding a new term to the sentiment dictionary\n",
    "new_afinn['rescue'] =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_afinn_sent(inputstring):\n",
    "    \n",
    "    sentcount =0\n",
    "    for word in inputstring.split():  \n",
    "        if word.rstrip('?:!.,;') in new_afinn:\n",
    "            sentcount = sentcount + new_afinn[word.rstrip('?:!.,;')]\n",
    "             \n",
    "    if (sentcount < 0):\n",
    "        sentiment = 'Negative'\n",
    "    elif (sentcount >0):\n",
    "        sentiment = 'Positive'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechdf['modified_afinn'] = speechdf[\"cleantext\"].apply(lambda x: new_afinn_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleantext</th>\n",
       "      <th>afinn</th>\n",
       "      <th>modified_afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          cleantext  \\\n",
       "8  we heard about american like firefighter david dahlberg. he is here with american too. david faced down walls of flame to rescue almost 60 children trapped at a california summer camp threatened by wildfires.   \n",
       "\n",
       "     afinn modified_afinn  \n",
       "8  Neutral       Positive  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing that the change to the dictionary changed the sentiment label.\n",
    "speechdf.iloc[8:9][['cleantext','afinn','modified_afinn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleantext</th>\n",
       "      <th>afinn</th>\n",
       "      <th>modified_afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we saw the volunteers of the \"cajun navy,\" racing to the rescue with their fishing boats to save people in the aftermath of a devastating hurricane.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              cleantext  \\\n",
       "5  we saw the volunteers of the \"cajun navy,\" racing to the rescue with their fishing boats to save people in the aftermath of a devastating hurricane.   \n",
       "\n",
       "      afinn modified_afinn  \n",
       "5  Positive       Positive  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing that the change to the dictionary had no effect on the sentiment label.\n",
    "speechdf.iloc[5:6][['cleantext','afinn','modified_afinn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleantext</th>\n",
       "      <th>afinn</th>\n",
       "      <th>modified_afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>today he lives in seoul, where he rescues other defectors, and broadcasts into north korea what the regime fears the most — the truth.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  cleantext  \\\n",
       "129  today he lives in seoul, where he rescues other defectors, and broadcasts into north korea what the regime fears the most — the truth.   \n",
       "\n",
       "        afinn modified_afinn  \n",
       "129  Positive       Positive  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing that the change to the dictionary had no effect on the sentiment label.\n",
    "speechdf.iloc[129:130][['cleantext','afinn','modified_afinn']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
